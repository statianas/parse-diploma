# parse-diploma
Реализация парсеров сайтов для создания датасета образовательных и научных текстов, написанных человеком и генерация текстов посредством легкой модели `qwen2.5vl:7b`. Фрагменты и цитирования текстов будут использованы в учебных целях.

## Как запускать
Для начала поставим необходимые библиотеки 
```console
parse-diploma:~$ pip install requirements.txt
```
### Парсер сайта [кафедры СП СПбГУ](https://se.math.spbu.ru/theses.html)
Для того, чтобы запустить парсер достаточно вызвать скрипт `parser_web.py`
```console
parse-diploma:~$ python3 parser_web.py
```
Этот скрипт:
- Принимает куки
- Ждёт загрузки работ в #ThesisList
- Для каждой работы берёт заголовок (внутри год + тема диплома), ссылку на PDF по XPath
- Скачивает pdf и обрабатывает в отдельном модуле `process_pdf.py`
- Меняет страницу в url.

### Парсер сайта [ВКР ВШЭ](https://www.hse.ru/edu/vkr/)
Для того, чтобы запустить парсер достаточно вызвать скрипт `parser_hse.py`
```console
parse-diploma:~$ python3 parser_hse.py --year <year> --faculty <code>
```
Соответствует поиску всех работ в **\<year\>** году факультета **\<code\>**. Коды факультетов можно посмотреть в файле `faculty_dict.json`

Этот скрипт проделывает те же действия, что и выше, но в процессе переходит на страницу с аннотацией и забирает pdf.

### Парсер сайта кафедры СП СПбГУ по id
Этот парсер создан для того, чтобы обходить битые страницы. Поскольку у каждой работы есть свой id, можно просто итерироваться по ним.

Для того, чтобы запустить парсер достаточно вызвать скрипт `parser_web_id.py`
```console
parse-diploma:~$ python3 parser_web_id.py <start_id> <end_id>
```
### Обработка текстов
Для того, чтобы фильтровать артефакты в полученных текстах pdf и в дальнейшем использовать тексты необходима обработка и разбиение на небольшие куски. Чтобы выполнить данные этапы необхоодимо запустить скрипт `partition_preprocess.py`
```console
parse-diploma:~$ python3 partition_preprocess.py --uni <'hse' or 'spbu'> --in-dir <in_dir> --out-dir <out_dir>
```
### Генерация текстов
Для создания второй части датасета - сгенерированных текстов (моделью `qwen2.5vl:7b`), мы продолжаем первое предложение реального текста до абзаца среднего размера (700-900 символом без учета пробелов). 

Чтобы начать генерацию необходимо запустить jupiter notebook `generate-text.ipynb` на kaggle. Достаточно последовательно запускать ячейки, изменяя такие параметры как **PROCESSED_DIR**, **OUTPUT_DIR**, **INPUT_DIR**. Также необходимо раскоментить нужную ячейку с этими параметрами в зависимости от того, какого типа тексты будут использованы для генерации.

### Utility скрипты

Для получения json **<output_json>** с уже обработанными id в директории **<downloads_dir>** необходимо запустить скрипт
```console
parse-diploma/:~$ python3 utility/ids-collection.py <downloads_dir> <output_json>
```

Для получения json **<output_json>** с уже обработанными именами файлов в директории **<downloads_dir>** необходимо запустить скрипт
```console
parse-diploma/:~$ python3 utility/name-collection.py <downloads_dir> <output_json>
```

Для получения нескольких рандомных файлов для ручного просмотра и поиска артефактов запустите скрипт `get-random.py`













